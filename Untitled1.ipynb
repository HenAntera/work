{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ta\\trend.py:608: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i]/self._trs[i])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ta\\trend.py:612: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i]/self._trs[i])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ta import add_all_ta_features\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from numpy import array\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from kerastuner.tuners import RandomSearch\n",
    "from kerastuner.engine.hyperparameters import HyperParameters\n",
    "import time\n",
    "import datetime\n",
    "from matplotlib import pyplot\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "#%%\n",
    "data = pd.read_csv(\"BTC.csv\",header=0)\n",
    "\n",
    "data[\"Unix Timestamp\"] = pd.to_datetime(data[\"Unix Timestamp\"],unit='s')\n",
    "data = data.sort_values(by=\"Unix Timestamp\")\n",
    "data = data.drop(labels={\"Date\",\"Symbol\", \"Volume BTC\"},axis=1)\n",
    "\n",
    "data = data.drop(data.index[:5678])\n",
    "data = data.drop(data.index[23590:])\n",
    "\n",
    "statdf = data.describe()\n",
    "\n",
    "dif = pd.read_csv(\"difficulty.csv\",header=0)\n",
    "dif['timestamp'] = pd.to_datetime(dif['timestamp'])\n",
    "dif = dif.set_index('timestamp')\n",
    "hrate = pd.read_csv(\"hash-rate.csv\",header=0)\n",
    "hrate['timestamp'] = pd.to_datetime(hrate['timestamp'])\n",
    "hrate = hrate.set_index('timestamp')\n",
    "market_cap = pd.read_csv(\"market-cap.csv\",header=0)\n",
    "market_cap['timestamp'] = pd.to_datetime(market_cap['timestamp'])\n",
    "market_cap = market_cap.set_index('timestamp')\n",
    "sopr = pd.read_csv(\"sopr.csv\",header=0)\n",
    "sopr['timestamp'] = pd.to_datetime(sopr['timestamp'])\n",
    "sopr = sopr.set_index('timestamp')\n",
    "fees_mean = pd.read_csv(\"fees-mean.csv\",header=0)\n",
    "fees_mean['timestamp'] = pd.to_datetime(fees_mean['timestamp'])\n",
    "fees_mean = fees_mean.set_index('timestamp')\n",
    "fees_total = pd.read_csv(\"fees-total.csv\",header=0)\n",
    "fees_total['timestamp'] = pd.to_datetime(fees_total['timestamp'])\n",
    "fees_total = fees_total.set_index('timestamp')\n",
    "act_addresses = pd.read_csv(\"active-addresses.csv\",header=0)\n",
    "act_addresses['timestamp'] = pd.to_datetime(act_addresses['timestamp'])\n",
    "act_addresses = act_addresses.set_index('timestamp')\n",
    "new_addresses = pd.read_csv(\"new-addresses.csv\",header=0)\n",
    "new_addresses['timestamp'] = pd.to_datetime(new_addresses['timestamp'])\n",
    "new_addresses = new_addresses.set_index('timestamp')\n",
    "tran_rate = pd.read_csv(\"transaction-rate.csv\",header=0)\n",
    "tran_rate['timestamp'] = pd.to_datetime(tran_rate['timestamp'])\n",
    "tran_rate = tran_rate.set_index('timestamp')\n",
    "tran_siz_mean = pd.read_csv(\"transaction-size-mean.csv\",header=0)\n",
    "tran_siz_mean['timestamp'] = pd.to_datetime(tran_siz_mean['timestamp'])\n",
    "tran_siz_mean = tran_siz_mean.set_index('timestamp')\n",
    "transfer_volume_mean = pd.read_csv(\"transfer-volume-mean.csv\",header=0)\n",
    "transfer_volume_mean['timestamp'] = pd.to_datetime(transfer_volume_mean['timestamp'])\n",
    "transfer_volume_mean = transfer_volume_mean.set_index('timestamp')\n",
    "market_cap_tether = pd.read_csv(\"market-cap-tether.csv\",header=0)\n",
    "market_cap_tether['timestamp'] = pd.to_datetime(market_cap_tether['timestamp'])\n",
    "market_cap_tether = market_cap_tether.set_index('timestamp')\n",
    "utxo_spent = pd.read_csv(\"utxo-value-spent-mean.csv\",header=0)\n",
    "utxo_spent['timestamp'] = pd.to_datetime(utxo_spent['timestamp'])\n",
    "utxo_spent = utxo_spent.set_index('timestamp')\n",
    "utxo_created = pd.read_csv(\"utxo-value-created-mean.csv\",header=0)\n",
    "utxo_created['timestamp'] = pd.to_datetime(utxo_created['timestamp'])\n",
    "utxo_created = utxo_created.set_index('timestamp')\n",
    "\n",
    "data = data.set_index('Unix Timestamp')\n",
    "data = data.set_index(dif.index)\n",
    "\n",
    "data[\"hrate\"] = hrate[\"value\"]\n",
    "data[\"difficulty\"] = dif[\"value\"]\n",
    "data[\"market_cap\"] = market_cap[\"value\"]\n",
    "data[\"sopr\"] = sopr[\"value\"]\n",
    "data[\"fees_mean\"] = fees_mean[\"value\"]\n",
    "data[\"fees_total\"] = fees_total[\"value\"]\n",
    "data[\"act_addresses\"] = act_addresses[\"value\"]\n",
    "data[\"new_addresses\"] = new_addresses[\"value\"]\n",
    "data[\"tran_rate\"] = tran_rate[\"value\"]\n",
    "data[\"tran_siz_mean\"] = tran_siz_mean[\"value\"]\n",
    "data[\"transfer_volume_mean\"] = transfer_volume_mean[\"value\"]\n",
    "data[\"market_cap_tether\"] = market_cap_tether[\"value\"]\n",
    "data[\"utxo_spent\"] = utxo_spent[\"value\"]\n",
    "data[\"utxo_created\"] = utxo_created[\"value\"]\n",
    "data[\"utxo_sum\"] = utxo_created[\"value\"] - utxo_spent[\"value\"]\n",
    "\n",
    "data = add_all_ta_features(\n",
    "    data, open=\"Open\", high=\"High\", low=\"Low\", close=\"Close\", volume=\"Volume USD\",fillna=True)\n",
    "\n",
    "data = data.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "lst = list()\n",
    "\n",
    "for i in data[\"others_dlr\"]:\n",
    "    if i <= 0 :\n",
    "        lst.append(int(0))\n",
    "    else:\n",
    "        lst.append(int(1))\n",
    "#%% Labelling\n",
    "data[\"classification\"] = lst\n",
    "data[\"classification\"] = data[[\"classification\"]].shift(-1)\n",
    "data[\"classification\"] = data[\"classification\"].fillna(0)\n",
    "#%% Clean\n",
    "data = data.fillna(0)\n",
    "data = data.drop(labels = {'High','Low','others_dr','others_dlr','others_cr'}, axis=1)\n",
    "dataset = data.values\n",
    "datasets = dataset[42:,:]\n",
    "\n",
    "#%%\n",
    "signal_test = np.load(\"y_predict.npy\")\n",
    "\n",
    "signal_test = np.reshape(signal_test, (4710,1))\n",
    "\n",
    "signal_train = datasets[:18838,[87]]\n",
    "\n",
    "signal = np.concatenate((signal_train, signal_test), axis=0)\n",
    "\n",
    "X = datasets[:,:86]\n",
    "#%%\n",
    "X = np.concatenate((X, signal), axis=1)\n",
    "\n",
    "#%%\n",
    "aaXX = np.delete(X, [6,7,20,33,34,35,35,41,42,44,55,56,57,58,76,77], axis=1)\n",
    "\n",
    "X = aaXX[:,[0, 1, 3, 5, 6, 13, 15, 17, 19, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, \n",
    "         35, 36, 38, 39, 40, 41, 42, 44, 51, 52, 53, 54, 55, 56, 57, 59, 60, 61, 62, 69, 71]]\n",
    "#%%Split\n",
    "\n",
    "X_trai = X[:18838]\n",
    "X_tes = X[18838:]\n",
    "\n",
    "ss = StandardScaler()\n",
    "X_trains = ss.fit_transform(X_trai)\n",
    "X_tests = ss.transform(X_tes)\n",
    "\n",
    "sc = MinMaxScaler()\n",
    "X_trains = sc.fit_transform(X_trains)\n",
    "X_tests = sc.transform(X_tests)\n",
    "#%% Label\n",
    "def split_sequences(sequences, n_steps):\n",
    "\tX = list()\n",
    "\tfor i in range(len(sequences)):\n",
    "\t\t# find the end of this pattern\n",
    "\t\tend_ix = i + n_steps\n",
    "\t\t# check if we are beyond the dataset\n",
    "\t\tif end_ix > len(sequences):\n",
    "\t\t\tbreak\n",
    "\t\t# gather input and output parts of the pattern\n",
    "\t\tseq_x = sequences[i:end_ix, :-1]\n",
    "\t\tX.append(seq_x)\n",
    "\t\t\n",
    "\treturn array(X)\n",
    "\n",
    "def ylabel(close, steps):\n",
    "   # print(len(close))\n",
    "    S = list()\n",
    "    final_result=0\n",
    "    #for i in range(len(close)-steps+1):\n",
    "    for i in range(len(close)):\n",
    "        # find the end of this pattern\n",
    "        end_i = i + steps\n",
    "        #print(end_i)\n",
    "        # check if we are beyond the dataset\n",
    "        #final_result=1\n",
    "        if end_i >= len(close):\n",
    "            break\n",
    "        for c in close[i:end_i]:\n",
    "            #print(c)\n",
    "            #print(close[end_i])\n",
    "            #print(len(close[i:end_i]))\n",
    "            if c >= close[i]*1.05:\n",
    "                final_result=1\n",
    "                break\n",
    "            if c <= close[i]*0.95:\n",
    "               final_result=0\n",
    "               break\n",
    "            #print(final_result)\n",
    "            #if final_result==1:\n",
    "            if close[end_i] > close[i]:\n",
    "               final_result=1\n",
    "            elif close[end_i] < close[i]:\n",
    "               final_result=0  \n",
    "            #print(final_result)\n",
    "        S.append(final_result)\n",
    "    #print(S)\n",
    "    return array(S)\n",
    "\n",
    "X_train = split_sequences(X_trains, 24)\n",
    "X_test = split_sequences(X_tests, 24)\n",
    "y_training = ylabel(X[:18839,[1]], 24)\n",
    "y_testing = ylabel(X[18837:,[1]], 24)\n",
    "#%%\n",
    "X_train = X_train[:-24]\n",
    "y_training = y_training[24:]\n",
    "X_test =X_test[:-24]\n",
    "y_testing = y_testing[24:]\n",
    "\n",
    "\n",
    "#y_train = tf.keras.utils.to_categorical(y_training, num_classes=3)\n",
    "#y_test = tf.keras.utils.to_categorical(y_testing, num_classes=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18791, 24, 39)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_testing.shape\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:688: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "431/588 [====================>.........] - ETA: 0s - loss: 0.7009 - accuracy: 0.40 - ETA: 10s - loss: 0.6964 - accuracy: 0.468 - ETA: 12s - loss: 0.6946 - accuracy: 0.493 - ETA: 12s - loss: 0.6937 - accuracy: 0.513 - ETA: 13s - loss: 0.6937 - accuracy: 0.509 - ETA: 13s - loss: 0.6934 - accuracy: 0.500 - ETA: 13s - loss: 0.6933 - accuracy: 0.506 - ETA: 13s - loss: 0.6930 - accuracy: 0.505 - ETA: 13s - loss: 0.6931 - accuracy: 0.503 - ETA: 14s - loss: 0.6929 - accuracy: 0.503 - ETA: 14s - loss: 0.6932 - accuracy: 0.504 - ETA: 14s - loss: 0.6937 - accuracy: 0.502 - ETA: 14s - loss: 0.6938 - accuracy: 0.502 - ETA: 14s - loss: 0.6936 - accuracy: 0.504 - ETA: 14s - loss: 0.6934 - accuracy: 0.508 - ETA: 14s - loss: 0.6933 - accuracy: 0.509 - ETA: 14s - loss: 0.6934 - accuracy: 0.509 - ETA: 14s - loss: 0.6935 - accuracy: 0.507 - ETA: 14s - loss: 0.6934 - accuracy: 0.511 - ETA: 14s - loss: 0.6933 - accuracy: 0.513 - ETA: 14s - loss: 0.6933 - accuracy: 0.509 - ETA: 14s - loss: 0.6933 - accuracy: 0.509 - ETA: 14s - loss: 0.6932 - accuracy: 0.510 - ETA: 14s - loss: 0.6933 - accuracy: 0.506 - ETA: 14s - loss: 0.6933 - accuracy: 0.503 - ETA: 14s - loss: 0.6933 - accuracy: 0.504 - ETA: 14s - loss: 0.6933 - accuracy: 0.502 - ETA: 14s - loss: 0.6933 - accuracy: 0.502 - ETA: 14s - loss: 0.6933 - accuracy: 0.501 - ETA: 14s - loss: 0.6934 - accuracy: 0.500 - ETA: 13s - loss: 0.6933 - accuracy: 0.502 - ETA: 13s - loss: 0.6933 - accuracy: 0.503 - ETA: 13s - loss: 0.6934 - accuracy: 0.503 - ETA: 13s - loss: 0.6933 - accuracy: 0.504 - ETA: 13s - loss: 0.6933 - accuracy: 0.504 - ETA: 13s - loss: 0.6934 - accuracy: 0.502 - ETA: 13s - loss: 0.6933 - accuracy: 0.503 - ETA: 13s - loss: 0.6934 - accuracy: 0.502 - ETA: 13s - loss: 0.6932 - accuracy: 0.505 - ETA: 13s - loss: 0.6933 - accuracy: 0.504 - ETA: 13s - loss: 0.6932 - accuracy: 0.506 - ETA: 13s - loss: 0.6933 - accuracy: 0.503 - ETA: 13s - loss: 0.6933 - accuracy: 0.504 - ETA: 13s - loss: 0.6933 - accuracy: 0.504 - ETA: 13s - loss: 0.6933 - accuracy: 0.503 - ETA: 13s - loss: 0.6933 - accuracy: 0.503 - ETA: 13s - loss: 0.6934 - accuracy: 0.502 - ETA: 13s - loss: 0.6935 - accuracy: 0.500 - ETA: 13s - loss: 0.6935 - accuracy: 0.499 - ETA: 13s - loss: 0.6935 - accuracy: 0.497 - ETA: 13s - loss: 0.6935 - accuracy: 0.498 - ETA: 13s - loss: 0.6935 - accuracy: 0.499 - ETA: 13s - loss: 0.6935 - accuracy: 0.498 - ETA: 13s - loss: 0.6935 - accuracy: 0.498 - ETA: 13s - loss: 0.6935 - accuracy: 0.497 - ETA: 12s - loss: 0.6935 - accuracy: 0.499 - ETA: 12s - loss: 0.6935 - accuracy: 0.500 - ETA: 12s - loss: 0.6934 - accuracy: 0.502 - ETA: 12s - loss: 0.6934 - accuracy: 0.503 - ETA: 12s - loss: 0.6934 - accuracy: 0.503 - ETA: 12s - loss: 0.6935 - accuracy: 0.504 - ETA: 12s - loss: 0.6935 - accuracy: 0.503 - ETA: 12s - loss: 0.6934 - accuracy: 0.504 - ETA: 12s - loss: 0.6934 - accuracy: 0.504 - ETA: 12s - loss: 0.6934 - accuracy: 0.505 - ETA: 12s - loss: 0.6934 - accuracy: 0.505 - ETA: 12s - loss: 0.6934 - accuracy: 0.507 - ETA: 12s - loss: 0.6934 - accuracy: 0.507 - ETA: 12s - loss: 0.6934 - accuracy: 0.507 - ETA: 11s - loss: 0.6933 - accuracy: 0.508 - ETA: 11s - loss: 0.6933 - accuracy: 0.508 - ETA: 11s - loss: 0.6934 - accuracy: 0.507 - ETA: 11s - loss: 0.6934 - accuracy: 0.507 - ETA: 11s - loss: 0.6934 - accuracy: 0.506 - ETA: 11s - loss: 0.6934 - accuracy: 0.505 - ETA: 11s - loss: 0.6934 - accuracy: 0.504 - ETA: 11s - loss: 0.6934 - accuracy: 0.505 - ETA: 11s - loss: 0.6934 - accuracy: 0.506 - ETA: 11s - loss: 0.6934 - accuracy: 0.506 - ETA: 11s - loss: 0.6934 - accuracy: 0.506 - ETA: 11s - loss: 0.6934 - accuracy: 0.504 - ETA: 11s - loss: 0.6934 - accuracy: 0.504 - ETA: 11s - loss: 0.6934 - accuracy: 0.504 - ETA: 10s - loss: 0.6934 - accuracy: 0.504 - ETA: 10s - loss: 0.6934 - accuracy: 0.503 - ETA: 10s - loss: 0.6933 - accuracy: 0.505 - ETA: 10s - loss: 0.6933 - accuracy: 0.507 - ETA: 10s - loss: 0.6933 - accuracy: 0.508 - ETA: 10s - loss: 0.6933 - accuracy: 0.507 - ETA: 10s - loss: 0.6933 - accuracy: 0.507 - ETA: 10s - loss: 0.6933 - accuracy: 0.508 - ETA: 10s - loss: 0.6933 - accuracy: 0.509 - ETA: 10s - loss: 0.6932 - accuracy: 0.509 - ETA: 10s - loss: 0.6932 - accuracy: 0.511 - ETA: 10s - loss: 0.6932 - accuracy: 0.511 - ETA: 10s - loss: 0.6932 - accuracy: 0.511 - ETA: 9s - loss: 0.6932 - accuracy: 0.512 - ETA: 9s - loss: 0.6932 - accuracy: 0.51 - ETA: 9s - loss: 0.6932 - accuracy: 0.51 - ETA: 9s - loss: 0.6932 - accuracy: 0.51 - ETA: 9s - loss: 0.6932 - accuracy: 0.51 - ETA: 9s - loss: 0.6932 - accuracy: 0.51 - ETA: 9s - loss: 0.6932 - accuracy: 0.51 - ETA: 9s - loss: 0.6932 - accuracy: 0.51 - ETA: 9s - loss: 0.6931 - accuracy: 0.51 - ETA: 9s - loss: 0.6930 - accuracy: 0.51 - ETA: 9s - loss: 0.6932 - accuracy: 0.51 - ETA: 9s - loss: 0.6931 - accuracy: 0.51 - ETA: 9s - loss: 0.6932 - accuracy: 0.51 - ETA: 9s - loss: 0.6931 - accuracy: 0.51 - ETA: 8s - loss: 0.6931 - accuracy: 0.51 - ETA: 8s - loss: 0.6932 - accuracy: 0.51 - ETA: 8s - loss: 0.6932 - accuracy: 0.51 - ETA: 8s - loss: 0.6932 - accuracy: 0.51 - ETA: 8s - loss: 0.6932 - accuracy: 0.51 - ETA: 8s - loss: 0.6931 - accuracy: 0.51 - ETA: 8s - loss: 0.6931 - accuracy: 0.51 - ETA: 8s - loss: 0.6931 - accuracy: 0.51 - ETA: 8s - loss: 0.6932 - accuracy: 0.51 - ETA: 8s - loss: 0.6932 - accuracy: 0.51 - ETA: 8s - loss: 0.6932 - accuracy: 0.51 - ETA: 8s - loss: 0.6932 - accuracy: 0.51 - ETA: 8s - loss: 0.6931 - accuracy: 0.51 - ETA: 8s - loss: 0.6931 - accuracy: 0.51 - ETA: 8s - loss: 0.6931 - accuracy: 0.51 - ETA: 8s - loss: 0.6931 - accuracy: 0.51 - ETA: 7s - loss: 0.6931 - accuracy: 0.51 - ETA: 7s - loss: 0.6931 - accuracy: 0.51 - ETA: 7s - loss: 0.6931 - accuracy: 0.51 - ETA: 7s - loss: 0.6931 - accuracy: 0.51 - ETA: 7s - loss: 0.6930 - accuracy: 0.51 - ETA: 7s - loss: 0.6931 - accuracy: 0.51 - ETA: 7s - loss: 0.6931 - accuracy: 0.51 - ETA: 7s - loss: 0.6931 - accuracy: 0.51 - ETA: 7s - loss: 0.6931 - accuracy: 0.51 - ETA: 7s - loss: 0.6931 - accuracy: 0.51 - ETA: 7s - loss: 0.6931 - accuracy: 0.51 - ETA: 7s - loss: 0.6931 - accuracy: 0.51 - ETA: 7s - loss: 0.6931 - accuracy: 0.51 - ETA: 7s - loss: 0.6930 - accuracy: 0.51 - ETA: 7s - loss: 0.6930 - accuracy: 0.51 - ETA: 6s - loss: 0.6930 - accuracy: 0.51 - ETA: 6s - loss: 0.6930 - accuracy: 0.51 - ETA: 6s - loss: 0.6930 - accuracy: 0.51 - ETA: 6s - loss: 0.6930 - accuracy: 0.51 - ETA: 6s - loss: 0.6930 - accuracy: 0.51 - ETA: 6s - loss: 0.6930 - accuracy: 0.51 - ETA: 6s - loss: 0.6930 - accuracy: 0.51 - ETA: 6s - loss: 0.6929 - accuracy: 0.51 - ETA: 6s - loss: 0.6929 - accuracy: 0.51 - ETA: 6s - loss: 0.6930 - accuracy: 0.51 - ETA: 6s - loss: 0.6929 - accuracy: 0.51 - ETA: 6s - loss: 0.6929 - accuracy: 0.51 - ETA: 6s - loss: 0.6929 - accuracy: 0.51 - ETA: 6s - loss: 0.6929 - accuracy: 0.51 - ETA: 6s - loss: 0.6929 - accuracy: 0.51 - ETA: 6s - loss: 0.6929 - accuracy: 0.51 - ETA: 5s - loss: 0.6929 - accuracy: 0.51 - ETA: 5s - loss: 0.6929 - accuracy: 0.51 - ETA: 5s - loss: 0.6929 - accuracy: 0.51 - ETA: 5s - loss: 0.6929 - accuracy: 0.51 - ETA: 5s - loss: 0.6929 - accuracy: 0.51 - ETA: 5s - loss: 0.6929 - accuracy: 0.51 - ETA: 5s - loss: 0.6929 - accuracy: 0.51 - ETA: 5s - loss: 0.6929 - accuracy: 0.51 - ETA: 5s - loss: 0.6929 - accuracy: 0.51 - ETA: 5s - loss: 0.6929 - accuracy: 0.51 - ETA: 5s - loss: 0.6929 - accuracy: 0.51 - ETA: 5s - loss: 0.6929 - accuracy: 0.51 - ETA: 5s - loss: 0.6929 - accuracy: 0.51 - ETA: 5s - loss: 0.6929 - accuracy: 0.51 - ETA: 5s - loss: 0.6929 - accuracy: 0.51 - ETA: 4s - loss: 0.6929 - accuracy: 0.51 - ETA: 4s - loss: 0.6928 - accuracy: 0.51 - ETA: 4s - loss: 0.6928 - accuracy: 0.51 - ETA: 4s - loss: 0.6928 - accuracy: 0.51 - ETA: 4s - loss: 0.6928 - accuracy: 0.51 - ETA: 4s - loss: 0.6928 - accuracy: 0.51 - ETA: 4s - loss: 0.6928 - accuracy: 0.51 - ETA: 4s - loss: 0.6927 - accuracy: 0.51 - ETA: 4s - loss: 0.6927 - accuracy: 0.51 - ETA: 4s - loss: 0.6926 - accuracy: 0.51 - ETA: 4s - loss: 0.6925 - accuracy: 0.51 - ETA: 4s - loss: 0.6925 - accuracy: 0.51 - ETA: 4s - loss: 0.6927 - accuracy: 0.51 - ETA: 4s - loss: 0.6927 - accuracy: 0.51 - ETA: 4s - loss: 0.6926 - accuracy: 0.51 - ETA: 4s - loss: 0.6927 - accuracy: 0.51 - ETA: 4s - loss: 0.6927 - accuracy: 0.51 - ETA: 4s - loss: 0.6927 - accuracy: 0.5179"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "588/588 [==============================] - ETA: 3s - loss: 0.6927 - accuracy: 0.51 - ETA: 3s - loss: 0.6927 - accuracy: 0.51 - ETA: 3s - loss: 0.6926 - accuracy: 0.51 - ETA: 3s - loss: 0.6926 - accuracy: 0.51 - ETA: 3s - loss: 0.6926 - accuracy: 0.51 - ETA: 3s - loss: 0.6927 - accuracy: 0.51 - ETA: 3s - loss: 0.6926 - accuracy: 0.51 - ETA: 3s - loss: 0.6926 - accuracy: 0.51 - ETA: 3s - loss: 0.6926 - accuracy: 0.51 - ETA: 3s - loss: 0.6926 - accuracy: 0.51 - ETA: 3s - loss: 0.6926 - accuracy: 0.51 - ETA: 3s - loss: 0.6926 - accuracy: 0.51 - ETA: 3s - loss: 0.6926 - accuracy: 0.51 - ETA: 3s - loss: 0.6926 - accuracy: 0.51 - ETA: 3s - loss: 0.6926 - accuracy: 0.51 - ETA: 3s - loss: 0.6926 - accuracy: 0.51 - ETA: 3s - loss: 0.6925 - accuracy: 0.51 - ETA: 3s - loss: 0.6925 - accuracy: 0.51 - ETA: 3s - loss: 0.6926 - accuracy: 0.51 - ETA: 2s - loss: 0.6926 - accuracy: 0.51 - ETA: 2s - loss: 0.6926 - accuracy: 0.51 - ETA: 2s - loss: 0.6926 - accuracy: 0.51 - ETA: 2s - loss: 0.6926 - accuracy: 0.51 - ETA: 2s - loss: 0.6926 - accuracy: 0.51 - ETA: 2s - loss: 0.6926 - accuracy: 0.51 - ETA: 2s - loss: 0.6926 - accuracy: 0.51 - ETA: 2s - loss: 0.6926 - accuracy: 0.51 - ETA: 2s - loss: 0.6926 - accuracy: 0.51 - ETA: 2s - loss: 0.6926 - accuracy: 0.51 - ETA: 2s - loss: 0.6926 - accuracy: 0.51 - ETA: 2s - loss: 0.6926 - accuracy: 0.51 - ETA: 2s - loss: 0.6926 - accuracy: 0.51 - ETA: 2s - loss: 0.6926 - accuracy: 0.51 - ETA: 2s - loss: 0.6926 - accuracy: 0.51 - ETA: 2s - loss: 0.6926 - accuracy: 0.51 - ETA: 2s - loss: 0.6926 - accuracy: 0.51 - ETA: 2s - loss: 0.6926 - accuracy: 0.51 - ETA: 2s - loss: 0.6926 - accuracy: 0.51 - ETA: 2s - loss: 0.6926 - accuracy: 0.51 - ETA: 1s - loss: 0.6926 - accuracy: 0.51 - ETA: 1s - loss: 0.6925 - accuracy: 0.51 - ETA: 1s - loss: 0.6926 - accuracy: 0.51 - ETA: 1s - loss: 0.6926 - accuracy: 0.51 - ETA: 1s - loss: 0.6925 - accuracy: 0.51 - ETA: 1s - loss: 0.6926 - accuracy: 0.51 - ETA: 1s - loss: 0.6926 - accuracy: 0.51 - ETA: 1s - loss: 0.6926 - accuracy: 0.51 - ETA: 1s - loss: 0.6925 - accuracy: 0.51 - ETA: 1s - loss: 0.6925 - accuracy: 0.51 - ETA: 1s - loss: 0.6925 - accuracy: 0.52 - ETA: 1s - loss: 0.6925 - accuracy: 0.51 - ETA: 1s - loss: 0.6925 - accuracy: 0.51 - ETA: 1s - loss: 0.6925 - accuracy: 0.51 - ETA: 1s - loss: 0.6925 - accuracy: 0.51 - ETA: 1s - loss: 0.6925 - accuracy: 0.51 - ETA: 1s - loss: 0.6925 - accuracy: 0.51 - ETA: 0s - loss: 0.6925 - accuracy: 0.51 - ETA: 0s - loss: 0.6925 - accuracy: 0.51 - ETA: 0s - loss: 0.6925 - accuracy: 0.51 - ETA: 0s - loss: 0.6925 - accuracy: 0.51 - ETA: 0s - loss: 0.6925 - accuracy: 0.51 - ETA: 0s - loss: 0.6925 - accuracy: 0.51 - ETA: 0s - loss: 0.6925 - accuracy: 0.51 - ETA: 0s - loss: 0.6925 - accuracy: 0.51 - ETA: 0s - loss: 0.6925 - accuracy: 0.51 - ETA: 0s - loss: 0.6925 - accuracy: 0.51 - ETA: 0s - loss: 0.6925 - accuracy: 0.51 - ETA: 0s - loss: 0.6925 - accuracy: 0.51 - ETA: 0s - loss: 0.6924 - accuracy: 0.51 - ETA: 0s - loss: 0.6924 - accuracy: 0.51 - ETA: 0s - loss: 0.6924 - accuracy: 0.51 - ETA: 0s - loss: 0.6925 - accuracy: 0.51 - ETA: 0s - loss: 0.6924 - accuracy: 0.51 - 15s 26ms/step - loss: 0.6924 - accuracy: 0.5193\n",
      "Best: 0.526477 using {'learning_rate': 0.001, 'input_units': 20, 'final_units': 50, 'drop_rate_2': 0, 'drop_rate': 0.2, 'dense_units': 10}\n"
     ]
    }
   ],
   "source": [
    "def build_model(input_units=34, dense_units=3, drop_rate=0, drop_rate_2=0, learning_rate=0.001, final_units=32):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(input_units, recurrent_dropout=0.1, input_shape=(X_train.shape[1:]), return_sequences=True, activation=\"relu\"))\n",
    "    model.add(Dropout(drop_rate))\n",
    "        \n",
    "    model.add(LSTM(final_units, input_shape=[(X_train.shape[1:])], activation=\"relu\"))\n",
    "    \n",
    "    model.add(Dense(dense_units, activation=\"relu\"))\n",
    "    model.add(Dropout(drop_rate_2))\n",
    "\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate, decay=1e-6)\n",
    "\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=(\"accuracy\"))\n",
    "\n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(build_fn=build_model)\n",
    "splits = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "input_units = [10, 20, 30, 50, 75]\n",
    "drop_rate = [0, 0.1, 0.2, 0.3]\n",
    "\n",
    "drop_rate_2 = [0, 0.1, 0.2, 0.3]\n",
    "\n",
    "final_units = [10, 20, 30, 50, 75]\n",
    "learning_rate = [1e-3, 1e-4, 1e-5]\n",
    "dense_units = [10, 15, 20, 25, 30]\n",
    "\n",
    "\n",
    "param_grid = dict(input_units=input_units, dense_units=dense_units, learning_rate=learning_rate,\n",
    "                 drop_rate=drop_rate, drop_rate_2=drop_rate_2, final_units=final_units)\n",
    "\n",
    "grid = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_jobs=-1, cv=splits)\n",
    "grid_result = grid.fit(X_train, y_training)\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.001,\n",
       " 'input_units': 30,\n",
       " 'final_units': 30,\n",
       " 'drop_rate_2': 0.2,\n",
       " 'drop_rate_1': 0.3,\n",
       " 'drop_rate': 0.2,\n",
       " 'dense_units': 100,\n",
       " 'add_units': 30}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_result.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5284210562705993"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_result.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass classes=[0 1 2], y=[0 1 1 ... 0 0 1] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(y_training),\n",
    "                                                 y_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.2830742 , 0.66542882, 1.39308456])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
